{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Main_ML_Code_Physio_Back.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abdalrahman9/back_physio_imu_ml/blob/main/Analysis/Main_ML_Code_Physio_Back.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxY1STu31lII"
      },
      "source": [
        "## Import Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnfEmnzw1kB_"
      },
      "source": [
        "from matplotlib import interactive, pyplot as plt\n",
        "import numpy as np\n",
        "import math #needed for definition of pi\n",
        "import pandas as pd\n",
        "import time, datetime, csv, signal\n",
        "import os\n",
        "import re\n",
        "from seglearn.transform import InterpLongToWide\n",
        "from seglearn.transform import FeatureRep, Segment, Interp\n",
        "from seglearn.pipe import Pype\n",
        "from bokeh.models import ColumnDataSource, Grid, LinearAxis, MultiLine, Plot, Range1d, LayoutDOM\n",
        "\n",
        "\n",
        "\n",
        "def signal_dataframe(folder):\n",
        "  items = folder.split('/')\n",
        "  session_name = items[1]+\"_\"+items[2]+\"_\"+items[3]+\"_\"+items[4]\n",
        "  '''###\n",
        "  MERGE CODE\n",
        "  '''###\n",
        "  for id_sensor , (sensor,sensor_name) in enumerate(zip([A,B,C,D,E,F,G,H],['A','B','C','D','E','F','G','H'])):\n",
        "    if 'reps' in items:\n",
        "        ending = \"_\"+items[6]+\".csv\"\n",
        "    else:\n",
        "        ending = \".csv\"\n",
        "\n",
        "\n",
        "    dfa = pd.read_csv(folder+session_name+'_'+sensor.replace(':','')+'_acc'+ending) #Reading the dataset in a dataframe using Pandas\n",
        "    dfg = pd.read_csv(folder+session_name+\"_\"+sensor.replace(':','')+\"_gyro\"+ending) #Reading the dataset in a dataframe using Pandas\n",
        "    dfm = pd.read_csv(folder+session_name+\"_\"+sensor.replace(':','')+\"_mag\"+ending) #Reading the dataset in a dataframe using Pandas\n",
        "    dfq = pd.read_csv(folder+session_name+\"_\"+sensor.replace(':','')+\"_quat\"+ending) #Reading the dataset in a dataframe using Pandas\n",
        "    dfp = pd.read_csv(folder+session_name+\"_\"+sensor.replace(':','')+\"_pres\"+ending) #Reading the dataset in a dataframe using Pandas\n",
        "\n",
        "    for signal_id , signal in enumerate([dfa,dfg,dfm,dfq,dfp]):\n",
        "      #Drop Time column\n",
        "      signal.drop(columns=['time (-07:00)'],inplace=True)\n",
        "\n",
        "      #Switch order of elapsed time and epoch\n",
        "      cols  = list(signal.columns)\n",
        "      cols[0], cols[1] = cols[1] , cols[0]\n",
        "      signal = signal[cols]\n",
        "\n",
        "      #Interpolate the data to have all signals to 25Hz\n",
        "      clf = Pype([('interp', Interp(1. / 25., categorical_target=True))])\n",
        "      # print(signal[cols].columns.values[1:])\n",
        "      # test = ([dfa.to_numpy(),dfg.to_numpy(),dfm.to_numpy(),dfq.to_numpy(),dfp.to_numpy()])\n",
        "      signal_arr, _ = clf.fit_transform([signal.to_numpy()],[0])\n",
        "      signal_arr = pd.DataFrame(data=signal_arr[0],columns=signal[cols].columns.values[1:])\n",
        "      if signal_id == 0:\n",
        "        dfa =  signal_arr\n",
        "      elif signal_id == 1:\n",
        "        dfg =  signal_arr\n",
        "      elif signal_id == 2:\n",
        "        dfm =  signal_arr\n",
        "      elif signal_id == 3:\n",
        "        dfq =  signal_arr\n",
        "      else:\n",
        "        dfp =  signal_arr\n",
        "\n",
        "    #Synchronizing and merging the signals together\n",
        "    test1a = pd.merge_asof(dfa, dfg, on='epoch (ms)')\n",
        "    test2a = pd.merge_asof(test1a, dfm, on='epoch (ms)')\n",
        "    test3a = pd.merge_asof(test2a, dfq, on='epoch (ms)')\n",
        "    test4a = pd.merge_asof(test3a, dfp, on='epoch (ms)')\n",
        "\n",
        "    df = test4a\n",
        "\n",
        "    #Add sensor name to each of its corresponding columns\n",
        "    for signal in sensor_type_col_labels:\n",
        "      for axis in signal:\n",
        "        df.rename(columns = {axis:axis + \" - \"+ sensor_name}, inplace = True)\n",
        "        \n",
        "    if id_sensor == 0:\n",
        "      full_df = df.copy()\n",
        "    else:\n",
        "      full_df = pd.merge_asof(full_df, df, on='epoch (ms)')\n",
        "\n",
        "  # Remove any NA rows and reset index before returning array\n",
        "  full_df.dropna(inplace=True)\n",
        "  full_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "  #Taring epochs to first epoch entry (eg. Epoch[0] = 0 ms) \n",
        "  full_df['epoch (ms)'] = ((full_df['epoch (ms)']).sub(full_df['epoch (ms)'][0])).div(1000)\n",
        "\n",
        "  #Convert dataframe to ndarray and return it\n",
        "  return full_df.to_numpy()\n",
        "\n",
        "\n",
        "################################### MAIN CODE ######################################\n",
        "#1 Laying Down\n",
        "#2 Sustained Extension\n",
        "#4 Sagittal Extension\n",
        "#5 Extension in Standing\n",
        "#6 Flexion in Lying\n",
        "#7 Flexion in Sitting\n",
        "#8 Flexion in Standing\n",
        "#9 Side Glide in Laying\n",
        "#10 Side Glide in Standing\n",
        "#11 Rotation Flexion Knees Together\n",
        "#12 Rotation Flexion One leg stretch\n",
        "#13 Other\n",
        "\n",
        "###Define Global variables and arrays###\n",
        "\n",
        "#Sensor MAC\n",
        "A = \"C7:E1:38:1F:C0:DE\"\n",
        "B = \"F4:04:52:A2:CB:59\"\n",
        "C = \"E3:62:1F:8B:81:B7\"\n",
        "D = \"E8:9C:A5:A3:8A:60\"\n",
        "E = \"F9:0E:1C:DA:D4:1D\"\n",
        "F = \"CD:A5:4D:78:A1:B4\"\n",
        "G = \"EF:AA:47:DC:45:44\"\n",
        "H = \"CD:78:F1:6B:D8:67\"\n",
        "\n",
        "#Channel names\n",
        "acc_col_label = ['X-Axis (g)','Y-Axis (g)','Z-Axis (g)']\n",
        "gyro_col_label =['X-Axis (deg/s)','Y-Axis (deg/s)','Z-Axis (deg/s)']\n",
        "mag_col_label = ['X-Axis (T)','Y-Axis (T)','Z-Axis (T)']\n",
        "pres_col_label = ['Pressure (Pa)']\n",
        "quat_col_label = ['W-Axis','X-Axis (i)','Y-Axis (j)','Z-Axis (k)'] #should be i j k\n",
        "sensor_type_col_labels = [acc_col_label,gyro_col_label,mag_col_label,quat_col_label,pres_col_label]\n",
        "\n",
        "#Selecting the Participant Data that wants to be analyzed \n",
        "subj_names = ['p0','p1','p2','p3','p4','p5','p6','p7','p8','p9','p10','p11','p12','p13','p14','p15','p16','p17','p18']\n",
        "\n",
        "#Defining the data dict that will house the imported data\n",
        "data = {'X': [], 'Subject': [], 'Exercise': [], 'Side': [], 'Rep': [], 'Sensor': [], 'Signal' : [], 'Axis': [], 'Y': []}\n",
        "\n",
        "#Specify data folder\n",
        "directory = \"Data/\"\n",
        "\n",
        "#Important the relevent data files into the data dict\n",
        "for subj in subj_names:\t#Enter Subject Folder\n",
        "    folder = directory + subj + \"/\"   #update folder string with new directory\n",
        "\n",
        "    #Loop only with newest session with \"[max(os.listdir(folder))]\"\n",
        "    for session in [s for s in [max(os.listdir(folder))] if os.path.isdir(folder+s)]:\t#Enter Session Folder\n",
        "        folder = directory + subj + \"/\" + session + \"/\"   #update folder string with new directory\n",
        "\n",
        "        #Loop through each exercise folder except for nonrelevent folders (eg. tar & test folders)\n",
        "        #Change which folders you want to exclude based on which data you want (Desire all data or only posture or exercise)\n",
        "        for exercise in [e for e in os.listdir(folder) if os.path.isdir(folder+e) and e!='tar' and e!='test' and e!='offset' #and e!='pg' and e!='pfg' and e!='pb']: \n",
        "                         and e!='e1' and e!='e2' and e!='e3' and e!='e4' and e!='e5' and e!='e6' and e!='e7' and e!='random']:\t#Enter Exercise Folder\n",
        "            folder = directory + subj + \"/\" + session + \"/\" + exercise + \"/\"    #update folder string with new directory\n",
        "\n",
        "            #Extract all Round Folders and select the newest round\n",
        "            round_folders = [r for r in os.listdir(folder) if os.path.isdir(folder+r)]\n",
        "            #First check 2 sided exercises\n",
        "            if exercise in ['e3','e4','e7']:  #Where there are 2 sides (L/R)\n",
        "                left = [place for place in round_folders if 'l' in place]\n",
        "                right = [right for right in round_folders if right not in left]\n",
        "                if not left:\n",
        "                    round_folders = [max(right)]\n",
        "                elif not right:\n",
        "                    round_folders = [max(left)]\n",
        "                else:\n",
        "                    round_folders = [max(left),max(right)]\n",
        "            #Next check One sided exercises\n",
        "            else:# One sided Exercises\n",
        "                round_folders = [max(round_folders)]\n",
        "\n",
        "            #Now enter Round folder\n",
        "            for rounds in round_folders:\t#Enter Round Folder\n",
        "                folder = directory + subj + \"/\" + session + \"/\" + exercise + \"/\" + rounds + \"/\"   #update folder string with new directory\n",
        "                \n",
        "                #Check if there is a reps folder\n",
        "                if os.path.isdir(folder+\"reps\"): #Check if there is a reps folder\n",
        "                    folder = directory + subj + \"/\" + session + \"/\" + exercise + \"/\" + rounds + \"/reps/\"\n",
        "                    for reps in [r for r in os.listdir(folder) if os.path.isdir(folder+r)]:\n",
        "                        folder = directory + subj + \"/\" + session + \"/\" + exercise + \"/\" + rounds + \"/reps/\" + reps + \"/\"\n",
        "                        \n",
        "                        #Extract data from files and label the entry with the correspoding data & tags\n",
        "                        all_signals = signal_dataframe(folder)\n",
        "                        data['X'].append(all_signals)\n",
        "                        data['Subject'].append((re.findall(r\"\\d+\", subj))[0])\n",
        "                        data['Exercise'].append(exercise)\n",
        "                        data['Rep'].append(int(reps)+1)\n",
        "\n",
        "                        #If exercise has 2 sides, put the applicable side\n",
        "                        if 'l' in rounds:\n",
        "                            data['Side'].append('Left')\n",
        "                        elif rounds.count('r') == 2:\n",
        "                            data['Side'].append('Right')\n",
        "                        else:\n",
        "                            data['Side'].append('NA')\n",
        "                        print(\"Saved \"+folder)\n",
        "\n",
        "                #For exercises with no reps (ex. pg & e1)\n",
        "                else:\t#No Reps Folder Found\n",
        "                      #Extract data from files and label the entry with the correspoding data & tags\n",
        "                      all_signals = signal_dataframe(folder)\n",
        "                      data['X'].append(all_signals)\n",
        "                      data['Subject'].append((re.findall(r\"\\d+\", subj))[0])\n",
        "                      data['Exercise'].append(exercise)\n",
        "                      data['Rep'].append(1)\n",
        "                      \n",
        "                      #If exercise has 2 sides, put the applicable side\n",
        "                      if 'l' in rounds:\n",
        "                          data['Side'].append('Left')\n",
        "                      elif rounds.count('r') == 2:\n",
        "                          data['Side'].append('Right')\n",
        "                      else:\n",
        "                          data['Side'].append('NA')\n",
        "                      print(\"Saved \"+folder)\n",
        "\n",
        "#Save all the labels into one key for easier manipulation later on\n",
        "data['Y'] = np.column_stack([data['Subject'], data['Exercise'], data['Side'], data['Rep']])\n",
        "\n",
        "#Output the sizes of the important data\n",
        "print(\"Finished Saving Files\")\n",
        "\n",
        "print(\"# X \" + str(len(data['X'])))\n",
        "print(\"# Side \" + str(len(data['Side'])))\n",
        "print(\"# Subject \" + str(len(data['Subject'])))\n",
        "print(\"# Exercise \" + str(len(data['Exercise'])))\n",
        "print(\"# Rep \" + str(len(data['Rep'])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyVoho87k_mP"
      },
      "source": [
        "#Save data file in directory for simpler usage later on\n",
        "np.save('posture_data.npy', data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4Grk5p6mEYt"
      },
      "source": [
        "#Load the data file if it already exists \n",
        "data = (np.load('posture_data.npy',allow_pickle=True))[()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYpOOndDImYP"
      },
      "source": [
        "## Feature Selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jboJHkyUOUmZ"
      },
      "source": [
        "import seglearn as sgl\n",
        "\n",
        "#Import all features from seglearn library\n",
        "d = sgl.feature_functions.all_features()\n",
        "\n",
        "#Delete features that are not applicable or don't work\n",
        "del d['hmean']  #all elements have to be positive to calc the harmonic mean -- condition not satisfied\n",
        "del d['gmean']  #all elements have to be positive to calc the harmonic mean -- condition not satisfied\n",
        "del d['emg_var']  #EMG var not applicable for data set/problem (IMU data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLVjKm1LlvgH"
      },
      "source": [
        "#These two Functions should work but I get an error of: \"dividing my zero\" .. trying to fix\n",
        "del d['corr'] #func not working....\n",
        "del d['hist4']  #func not working...."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIWNyLDv6Mv0"
      },
      "source": [
        "def corr2(X):\n",
        "    \"\"\" computes correlations between all variable pairs in a segmented time series\n",
        "\n",
        "    .. note:: this feature is expensive to compute with the current implementation, and cannot be\n",
        "    used with univariate time series\n",
        "    \"\"\"\n",
        "    # print(X.shape)\n",
        "    X = np.atleast_3d(X)\n",
        "    # print(X.shape)\n",
        "    N = X.shape[0]\n",
        "    D = X.shape[2]\n",
        "\n",
        "    if D == 1:\n",
        "        return np.zeros(N, dtype=np.float)\n",
        "\n",
        "    trii = np.triu_indices(D, k=1) \n",
        "    DD = len(trii[0])\n",
        "    r = np.zeros((N, DD))\n",
        "    for i in np.arange(N):\n",
        "        rmat = np.corrcoef(X[i].transpose())  # get the ith window from each signal, result will be DxD\n",
        "        r[i] = rmat[trii]\n",
        "    return r\n",
        "\n",
        "#Replace old corr function with this altered version\n",
        "d['corr'] = corr2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djM_iccl1uM7"
      },
      "source": [
        "## Pipeline Creation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVOXpOHqvnwg"
      },
      "source": [
        "Creation of sensor/signal selection class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_S4_GA9uwNo"
      },
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "#Class for selecting specific Sensors and Signals from the original data\n",
        "class Sensor_Select(BaseEstimator,TransformerMixin):\n",
        "   'Common base class for sensor/signal Selection'\n",
        "   #Declare Sensor Indices within the 113 channel columns (originally 112 but epoch column was added thus 113)\n",
        "   A = 1\n",
        "   B = 15\n",
        "   C = 29\n",
        "   D = 43\n",
        "   E = 57\n",
        "   F = 71\n",
        "   G = 85\n",
        "   H = 99\n",
        "\n",
        "   #Declare the relative Signal indices within each sensor segment -> 14 signals within each sensor \n",
        "   acc = [0 , 1 , 2]\n",
        "   gyro = [3 , 4 , 5]\n",
        "   mag = [6, 7, 8]\n",
        "   quat = [9, 10, 11, 12]\n",
        "   pres = [13]\n",
        "\n",
        "   def __init__(self, sensors=[], signals=[]):\n",
        "     #Init the sensor, signal arrays\n",
        "     print(\"Entered Init\")\n",
        "     self.sensors = sensors\n",
        "     self.signals = signals\n",
        "     \n",
        "     #init an empty list to hold the respective indices for the selected sensors and signals\n",
        "     self.sensors_int = []\n",
        "     self.signals_int = []\n",
        "   \n",
        "   def fit(self,X,y=None):\n",
        "     print(\"Entered Fit\")\n",
        "     return self\n",
        "\n",
        "   def transform(self,X,y=None):\n",
        "     print(\"Entered transform\")\n",
        "\n",
        "     #Loop through the desired sensors and extract their respective indices within the 113 columns\n",
        "     for dev , indices in zip(['A','B','C','D','E','F','G','H'],[[self.A],[self.B],[self.C],[self.D],[self.E],[self.F],[self.G],[self.H]]):\n",
        "       if dev in self.sensors: self.sensors_int.extend(indices)\n",
        "\n",
        "     #Now loop through the desired signals and extract their respective indices within each sensor segment\n",
        "     for ch , relative_indices in zip(['acc','gyro','mag','quat','pres'], [self.acc,self.gyro,self.mag,self.quat,self.pres]):\n",
        "       if ch in self.signals: self.signals_int.extend(relative_indices)\n",
        "       \n",
        "     #Now Combine both sensor and signal indices to extract all the desired signals from each sensor\n",
        "     #I included column '0' by default within the list because it corresponds to the 'epoch' column and must\n",
        "     #be included no matter what sensors/signals are desired\n",
        "     Filtered_Indices = [0] + [i+ j for i in self.sensors_int for j in self.signals_int]\n",
        "          \n",
        "     #Now insert the desired indices (ie. columns/signals) within the main data and retrieve the desired sensors along with their signals\n",
        "     filtered_data = [entry[:,Filtered_Indices] for entry in X]\n",
        "\n",
        "     #Return the filtered data\n",
        "     return filtered_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ps143uLovtOM"
      },
      "source": [
        "Running Pipe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cv_ZaMOX1sB4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c96fed27-5856-44cc-870b-e294014e75bb"
      },
      "source": [
        "import seglearn as sgl\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "#Create the pype with all the main processing elements\n",
        "clf = sgl.Pype([('sensors', Sensor_Select(['A','B','C','D','E','F','G','H'],['acc','gyro','mag','quat','pres'])),\n",
        "                ('interp', Interp(1. / 25., categorical_target=True)),\n",
        "                ('segment', sgl.Segment(width=25,overlap=0.3)),\n",
        "                ('ftr', sgl.FeatureRep(features=d)),\n",
        "                ('scaler', StandardScaler())])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/home/abdallah/.local/lib/python3.8/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass memory=None as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
            "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGKGbIuppnAq",
        "outputId": "22817cb4-95ee-48d1-e88e-ad5fecbb6c1d"
      },
      "source": [
        "#Run the data through the pipe\n",
        "X, y = clf.fit_transform(data['X'], data['Y'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/home/abdallah/.local/lib/python3.8/site-packages/seglearn/transform.py:237: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  Xt = np.array([sliding_tensor(Xt[i], self.width, self._step, self.order)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyO21MkBUZ9H"
      },
      "source": [
        "## Cross Validation and Grid Searching"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-vP6zKHUifJ"
      },
      "source": [
        "#Import important libraries and functions\n",
        "import itertools\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "import pandas as pd\n",
        "import seglearn as sgl\n",
        "from seglearn.transform import FeatureRep, Segment, Interp\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from tensorflow.python.keras.layers import Dense, LSTM, Conv1D\n",
        "from tensorflow.python.keras.models import Sequential\n",
        "from tensorflow.python.keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from tensorflow.python.keras.utils import np_utils\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, log_loss\n",
        "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
        "# from sklearn.metrics import plot_confusion_matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "from sklearn.metrics import f1_score, make_scorer, balanced_accuracy_score\n",
        "from sklearn.model_selection import train_test_split, cross_validate\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import (TimeSeriesSplit, KFold, ShuffleSplit,\n",
        "                                     StratifiedKFold, GroupShuffleSplit,\n",
        "                                     GroupKFold, StratifiedShuffleSplit)\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZBJd-nRUusn"
      },
      "source": [
        "#Define 2 main Functions used for Grid Searching and Cross Validation\n",
        "\n",
        "#1 - The Plot confusion matrix function that takes in pred and true labels and \n",
        "# outputs the corresponding confusion matrix -- originally implemented by David \n",
        "# burns in his Seglearn Library\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=True,\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\" plots confusion matrix \"\"\"\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "#2 - The Plot Grid Search function takes the results of a grid search that had only \n",
        "# 2 hyperparameters that were tunes and outputs the relationship between the 2 hyperparameters\n",
        "# and the model's accuracy in the form of a plot\n",
        "def plot_grid_search(cv_results, grid_param_1, grid_param_2, name_param_1, name_param_2):\n",
        "    # plotting grid results from David Alvarez on Stack Overflow\n",
        "\n",
        "    # Get Test Scores Mean and std for each grid search\n",
        "    scores_mean = cv_results['mean_test_score']\n",
        "    scores_mean = np.array(scores_mean).reshape(len(grid_param_2), len(grid_param_1))\n",
        "    scores_sd = cv_results['std_test_score']\n",
        "    scores_sd = np.array(scores_sd).reshape(len(grid_param_2), len(grid_param_1))\n",
        "\n",
        "    # Plot Grid search scores\n",
        "    _, ax = plt.subplots(1, 1)\n",
        "\n",
        "    # Param1 is the X-axis, Param 2 is represented as a different curve (color line)\n",
        "    for idx, val in enumerate(grid_param_2):\n",
        "        ax.plot(grid_param_1, scores_mean[idx, :], '-o', label=name_param_2 + ': ' + str(val))\n",
        "\n",
        "    ax.set_title(\"Grid Search Scores\", fontsize=20, fontweight='bold')\n",
        "    ax.set_xlabel(name_param_1, fontsize=16)\n",
        "    ax.set_ylabel('CV Average Score', fontsize=16)\n",
        "    ax.legend(loc=\"best\", fontsize=15)\n",
        "    ax.grid(True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ze5UwKTPWJCy"
      },
      "source": [
        "Cross Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VcB_LabKWMBK"
      },
      "source": [
        "def run_model(model, alg_name, notation):\n",
        "    # # use subject id to group folds\n",
        "    splitter = GroupKFold(n_splits=n_splits)\n",
        "    cvk = splitter.split(X,y[:,1],groups=y[:,0])\n",
        "    cvk_list = list(cvk)\n",
        "\n",
        "    ##############################################################\n",
        "    # Cross Validation and Calculate Multiple Metrics\n",
        "    cv_scores = cross_validate(model, X[:,:],y[:,1], groups=y[:,0], cv=cvk_list, return_train_score=True, return_estimator=True)#scoring=scoring) \n",
        "\n",
        "    # Predictions\n",
        "    y_pred = np.empty(y[:,1].shape[0],dtype=y[:,1].dtype.str)\n",
        "    for idx, fold_model in enumerate(cv_scores['estimator']):\n",
        "      train_idx,test_idx = cvk_list[idx]#next(cvk)\n",
        "      y_pred[test_idx] = fold_model.predict(X[test_idx])\n",
        "\n",
        "    ##############################################################\n",
        "    # # Output Results\n",
        "    print(alg_name)\n",
        "\n",
        "    pd_cv = pd.DataFrame(cv_scores)\n",
        "\n",
        "    print(\"CV Average Scores: \")\n",
        "    for metric in pd_cv.loc[:,pd_cv.columns != 'estimator']:\n",
        "      print(metric + \": %0.2f (+/- %0.2f)\" % (pd_cv[metric].mean(), pd_cv[metric].std() * 2))\n",
        "\n",
        "    print(\"\\nCV Iteration Scores: \")  \n",
        "    print(pd_cv[['fit_time','test_score','train_score']])\n",
        "    print('\\n')\n",
        "\n",
        "    print(\"Classification Report: \")\n",
        "    print(classification_report(y[:,1], y_pred))\n",
        "\n",
        "    print(\"Confusion Matrix: \")\n",
        "    cm = confusion_matrix(y[:,1], y_pred)\n",
        "    plot_confusion_matrix(cm,np.unique(y[:,1]))\n",
        "\n",
        "    return cv_scores\n",
        "\n",
        "##############################################################################\n",
        "\n",
        "n_splits = 6\n",
        "cv_models = {}\n",
        "\n",
        "# ----- Random Forest ---------------\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "model = RandomForestClassifier(bootstrap=False,\n",
        " max_depth= 57,\n",
        " max_features='auto',\n",
        " min_samples_leaf= 1,\n",
        " min_samples_split= 20,\n",
        " n_estimators= 100)\n",
        "    #n_estimators = 100, min_samples_split = 10, min_samples_leaf = 4, bootstrap=False, max_features='sqrt')\n",
        "name = 'rf'\n",
        "cv_models[name] = run_model(model, \"Random Forest\", 'rf')\n",
        "\n",
        "# # ----- xgboost ------------\n",
        "# # install xgboost\n",
        "# # 'pip install xgboost' or https://stackoverflow.com/questions/33749735/how-to-install-xgboost-package-in-python-windows-platform/39811079#39811079\n",
        "# from xgboost import XGBClassifier\n",
        "\n",
        "# model = XGBClassifier()\n",
        "# notation = 'xg'\n",
        "# cv_models[notation] = run_model(model, \"XGBoost\", 'xg')\n",
        "\n",
        "#############################################\n",
        "#Save cv_models and results in object file for future reference in needed\n",
        "# np.save('/home/abdallah/Documents/Cross_Validate_Best_3_Models_Segment_exercise_no_hist_corr.npy', cv_models)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vm4AYp4xWLgW"
      },
      "source": [
        "Grid Searching"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsZ-rJcBWvKR"
      },
      "source": [
        "#######################################GRID SEARCHING ###################################################################\n",
        "def plot_grid_search(cv_results, grid_param_1, grid_param_2, name_param_1, name_param_2):\n",
        "    # plotting grid results from David Alvarez on Stack Overflow\n",
        "\n",
        "    # Get Test Scores Mean and std for each grid search\n",
        "    scores_mean = cv_results['mean_test_score']\n",
        "    scores_mean = np.array(scores_mean).reshape(len(grid_param_2), len(grid_param_1))\n",
        "    scores_sd = cv_results['std_test_score']\n",
        "    scores_sd = np.array(scores_sd).reshape(len(grid_param_2), len(grid_param_1))\n",
        "\n",
        "    # Plot Grid search scores\n",
        "    _, ax = plt.subplots(1, 1)\n",
        "\n",
        "    # Param1 is the X-axis, Param 2 is represented as a different curve (color line)\n",
        "    for idx, val in enumerate(grid_param_2):\n",
        "        ax.plot(grid_param_1, scores_mean[idx, :], '-o', label=name_param_2 + ': ' + str(val))\n",
        "\n",
        "    ax.set_title(\"Grid Search Scores\", fontsize=20, fontweight='bold')\n",
        "    ax.set_xlabel(name_param_1, fontsize=16)\n",
        "    ax.set_ylabel('CV Average Score', fontsize=16)\n",
        "    ax.legend(loc=\"best\", fontsize=15)\n",
        "    ax.grid(True)\n",
        "\n",
        "def run_model(model, alg_name, notation):\n",
        "    ##############################################################\n",
        "    # # Output Model Name\n",
        "    print(alg_name)\n",
        "\n",
        "    splitter = GroupKFold(n_splits=n_splits)\n",
        "    cvk = splitter.split(data['X'],data['Y'][:,1],groups=data['Y'][:,0])\n",
        "    cvk_list = list(cvk)\n",
        "\n",
        "    # clf = sgl.Pype([#('sensors', Sensor_Select(['A','B','C','D','E','F','G','H'],['acc','gyro','mag','quat','pres'])),\n",
        "    #             ('interp', Interp(1. / 25., categorical_target=True)),\n",
        "    #             ('seg', sgl.Segment()),\n",
        "    #             ('ftr', sgl.FeatureRep(features=d)),\n",
        "    #             ('scaler', StandardScaler()),\n",
        "    #             (notation , model)])\n",
        "    \n",
        "    #################Segment Hyperparameters#######################\n",
        "    # par_grid = {'seg__width': [12 ,25, 50, 75, 100, 125],\n",
        "    #         'seg__overlap': [0., 0.2, 0.4, 0.6]}\n",
        "\n",
        "    ################# Random Forest Hyperparameters #######################\n",
        "    # Number of trees in random forest\n",
        "    n_estimators = [int(x) for x in np.linspace(start = 100, stop = 2000, num = 6)]\n",
        "    # Number of features to consider at every split\n",
        "    max_features = ['auto', 'sqrt']\n",
        "    # Maximum number of levels in tree\n",
        "    max_depth = [int(x) for x in np.linspace(10, 200, num = 5)]\n",
        "    max_depth.append(None)\n",
        "    # Minimum number of samples required to split a node\n",
        "    min_samples_split = [2, 10, 20, 30]\n",
        "    # Minimum number of samples required at each leaf node\n",
        "    min_samples_leaf = [1, 2, 4, 8, 15]\n",
        "    # Method of selecting samples for training each tree\n",
        "    bootstrap = [True, False]\n",
        "    # Create the random grid\n",
        "    # par_grid = {'n_estimators': n_estimators,\n",
        "    #               'max_features': max_features,\n",
        "    #               'max_depth' : max_depth,\n",
        "    #               'min_samples_split': min_samples_split,\n",
        "    #               'min_samples_leaf': min_samples_leaf,\n",
        "    #               'bootstrap': bootstrap}\n",
        "\n",
        "    ################# XGBoost Hyperparameters #######################\n",
        "    par_grid = {\n",
        "    'learning_rate': [0.05, 0.10, 0.20, 0.30],\n",
        "    'max_depth': [3, 7, 15],\n",
        "    'min_child_weight': [1, 4, 7],\n",
        "    'subsample': [0.5, 0.7],\n",
        "    'gamma' : [ 0.0, 0.2 , 0.4 ],\n",
        "    'colsample_bytree': [0.3, 0.5, 0.7],\n",
        "    'n_estimators' : [100, 200, 500],\n",
        "    'objective': ['reg:squarederror'] }\n",
        "\n",
        "    #Grid Searching through the parameters\n",
        "    gs = GridSearchCV(estimator = model, param_grid = par_grid, cv=cvk_list, n_jobs = -1,return_train_score=True) #scoring='f1_weighted', return_train_score=True,\n",
        "    # gs = RandomizedSearchCV(estimator = model, param_distributions = par_grid, n_iter = 100, cv = cvk_list, random_state=42, n_jobs = -1, return_train_score=True)\n",
        "\n",
        "    gs.fit(X, y[:,1], groups = y[:,0])\n",
        "\n",
        "\n",
        "    return gs\n",
        "\n",
        "##############################################################################\n",
        "n_splits = 6\n",
        "cv_models = {}\n",
        "\n",
        "# ----- Random Forest ---------------\n",
        "# from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# model = RandomForestClassifier()\n",
        "# name = 'rf'\n",
        "# cv_models[name] = run_model(model, \"Random Forest\", 'rf')\n",
        "\n",
        "# # ----- xgboost ------------\n",
        "# # install xgboost\n",
        "# # 'pip install xgboost' or https://stackoverflow.com/questions/33749735/how-to-install-xgboost-package-in-python-windows-platform/39811079#39811079\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "model = XGBClassifier()\n",
        "notation = 'xg'\n",
        "cv_models[notation] = run_model(model, \"XGBoost\", 'xg')\n",
        "\n",
        "#############################################\n",
        "#Save cv_models and results in object file for future reference in needed\n",
        "np.save('/home/abdallah/Documents/Grid_Search_XGBoost_exercise_no_hist_corr.npy', cv_models)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}